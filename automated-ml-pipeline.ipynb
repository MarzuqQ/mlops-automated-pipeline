{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df9be90c-4355-43cf-b207-f6e8199acb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.224.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.34.84)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.21.1)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.1.0)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.66.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.84 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.34.84)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2660fb7c-61d8-460c-b435-7cebe6d14372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    Processor,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "\n",
    "from sagemaker import Model\n",
    "from sagemaker.xgboost import XGBoostPredictor\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource,\n",
    "    ModelMetrics,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import ProcessingStep, CacheConfig, TuningStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "\n",
    "from sagemaker.workflow.functions import Join, JsonGet\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.tuner import (\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    "    WarmStartConfig,\n",
    "    WarmStartTypes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80b40b3f-0e26-46d6-ad83-44ac25c48023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-211343875790\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import boto3  \n",
    "import sagemaker  \n",
    "from sagemaker.workflow.pipeline_context import PipelineSession  \n",
    "import pandas as pd  \n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()             # Initialize a SageMaker session\n",
    "region = sagemaker_session.boto_region_name                 # Get the AWS region\n",
    "role = sagemaker.get_execution_role()                       # Get the execution role for SageMaker\n",
    "pipeline_session = PipelineSession()                        # Initialize a pipeline session\n",
    "default_bucket = sagemaker_session.default_bucket()         # Get the default S3 bucket for SageMaker\n",
    "model_package_group_name = f\"AutoMpgPackageGroupName\"       # Define the model package group name\n",
    "\n",
    "print(default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533e4211-f4c8-4df2-9454-d03d1015330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66da2dbd-3b43-41cc-b381-18d84242bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-211343875790/auto-mpg/auto-mpg-dataset.csv\n"
     ]
    }
   ],
   "source": [
    "local_path = \"data/auto-mpg-dataset.csv\"  # Local path to save the downloaded dataset\n",
    "\n",
    "# Create an S3 resource object\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "# Download the dataset from your S3 bucket with bucket name and file name\n",
    "s3.Bucket(sagemaker_session.default_bucket()).download_file(\n",
    "    \"auto-mpg.csv\", local_path  \n",
    ")\n",
    "\n",
    "# Define the base URI for the uploaded dataset in your bucket\n",
    "base_uri = f\"s3://{default_bucket}/auto-mpg\"\n",
    "\n",
    "# Upload the local dataset to the defined S3 URI\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,  \n",
    "    desired_s3_uri=base_uri,  \n",
    ")\n",
    "\n",
    "# Print the S3 URI where the dataset was uploaded\n",
    "print(input_data_uri)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e57c5ee4-4d3c-4209-be9b-b26b61334a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a959795e-d0fb-4f29-85b4-4835671089dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
       "0  18.0          8         307.0        130    3504          12.0          70   \n",
       "1  15.0          8         350.0        165    3693          11.5          70   \n",
       "2  18.0          8         318.0        150    3436          11.0          70   \n",
       "3  16.0          8         304.0        150    3433          12.0          70   \n",
       "4  17.0          8         302.0        140    3449          10.5          70   \n",
       "\n",
       "   origin                   car name  \n",
       "0       1  chevrolet chevelle malibu  \n",
       "1       1          buick skylark 320  \n",
       "2       1         plymouth satellite  \n",
       "3       1              amc rebel sst  \n",
       "4       1                ford torino  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b92543c-103d-40e5-82d7-342f8888f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "ParameterInteger,\n",
    "ParameterString,\n",
    "ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\") \n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\" )\n",
    "base_job_prefix = \"ab3-autompg-example\"\n",
    "input_data = ParameterString(name=\"InputData\", default_value=input_data_uri,)\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=46.0)\n",
    "# Cache Pipeline steps to reduce execution time on subsequent executions\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"30d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2b8c8d3-4dff-477d-b588-2a7a6b7299b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8836dffd-a67c-4c89-9243-703f30a7584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile code/preprocess.py\n",
    "# import argparse\n",
    "# import logging\n",
    "# import os\n",
    "# import pathlib\n",
    "# import boto3\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "# logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# # Define column names and data types for the Auto MPG dataset\n",
    "# feature_columns_names = [\n",
    "#     \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"origin\"\n",
    "# ]\n",
    "# label_column = \"mpg\"\n",
    "\n",
    "# feature_columns_dtype = {\n",
    "#     \"cylinders\": np.float64,  # Change to float64 to handle NA values before converting to int\n",
    "#     \"displacement\": np.float64,\n",
    "#     \"horsepower\": np.float64,\n",
    "#     \"weight\": np.float64,\n",
    "#     \"acceleration\": np.float64,\n",
    "#     \"model_year\": np.float64,  # Change to float64 to handle NA values before converting to int\n",
    "#     \"origin\": str,\n",
    "# }\n",
    "# label_column_dtype = {\"mpg\": np.float64}\n",
    "\n",
    "# def merge_two_dicts(x, y):\n",
    "#     \"\"\"Merges two dicts, returning a new copy.\"\"\"\n",
    "#     z = x.copy()\n",
    "#     z.update(y)\n",
    "#     return z\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     logger.debug(\"Starting preprocessing.\")\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--input-data\", type=str, required=True)\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     base_dir = \"/opt/ml/processing\"\n",
    "#     pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)  # Create directories for input and output\n",
    "#     input_data = args.input_data\n",
    "#     bucket = input_data.split(\"/\")[2]\n",
    "#     key = \"/\".join(input_data.split(\"/\")[3:])\n",
    "\n",
    "#     logger.info(\"Downloading data from bucket: %s, key: %s\", bucket, key)\n",
    "#     fn = f\"{base_dir}/data/auto-mpg-dataset.csv\"\n",
    "#     s3 = boto3.resource(\"s3\")\n",
    "#     s3.Bucket(bucket).download_file(key, fn)  # Download the dataset from S3\n",
    "\n",
    "#     logger.debug(\"Reading downloaded data.\")\n",
    "#     df = pd.read_csv(fn, header=None, names=feature_columns_names)  # Read the CSV file into a DataFrame\n",
    "#     os.unlink(fn)  # Delete the local file after reading\n",
    "\n",
    "#     # Ensure 'horsepower' is handled correctly\n",
    "#     logger.debug(\"Handling missing values in 'horsepower' column.\")\n",
    "#     df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')  # Convert 'horsepower' to numeric, coerce errors to NaN\n",
    "\n",
    "#     # Convert non-numeric values to NaN\n",
    "#     for col in feature_columns_dtype.keys():\n",
    "#         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "#     # Handle missing values before converting data types\n",
    "#     df = df.fillna(df.median())  # Fill NaN values with the median\n",
    "\n",
    "#     # Convert columns to the appropriate data types\n",
    "#     for col, dtype in feature_columns_dtype.items():\n",
    "#         df[col] = df[col].astype(dtype)\n",
    "\n",
    "#     logger.debug(\"Defining transformers.\")\n",
    "#     features = list(feature_columns_names)\n",
    "#     features.remove(label_column)  # Remove the label column from the features list\n",
    "\n",
    "#     numeric_features = [name for name in features if df.dtypes[name] != 'object']  # Identify numeric features\n",
    "#     categorical_features = [name for name in features if df.dtypes[name] == 'object']  # Identify categorical features\n",
    "\n",
    "#     numeric_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values with median\n",
    "#             (\"scaler\", StandardScaler()),  # Standardize the numeric features\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     categorical_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),  # Impute missing values with 'missing'\n",
    "#             (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),  # One-hot encode categorical features\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     preprocess = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", numeric_transformer, numeric_features),  # Apply numeric transformations\n",
    "#             (\"cat\", categorical_transformer, categorical_features),  # Apply categorical transformations\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     logger.info(\"Applying transforms.\")\n",
    "#     y = df.pop(label_column)  # Separate the label from the features\n",
    "#     X_pre = preprocess.fit_transform(df)  # Fit and transform the features\n",
    "#     y_pre = y.to_numpy().reshape(len(y), 1)  # Reshape the label array\n",
    "\n",
    "#     X = np.concatenate((y_pre, X_pre), axis=1)  # Concatenate the label and features\n",
    "\n",
    "#     logger.info(\"Splitting %d rows of data into train, validation, test datasets.\", len(X))\n",
    "#     np.random.shuffle(X)  # Shuffle the dataset\n",
    "#     train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])  # Split the dataset\n",
    "\n",
    "#     # Further split the test set into test_set_1 and test_set_2\n",
    "#     test_set_1, test_set_2 = np.split(test, [int(0.5 * len(test))])\n",
    "\n",
    "#     logger.info(\"Writing out datasets to %s.\", base_dir)\n",
    "#     pathlib.Path(f\"{base_dir}/train\").mkdir(parents=True, exist_ok=True)\n",
    "#     pathlib.Path(f\"{base_dir}/validation\").mkdir(parents=True, exist_ok=True)\n",
    "#     pathlib.Path(f\"{base_dir}/test_set_1\").mkdir(parents=True, exist_ok=True)\n",
    "#     pathlib.Path(f\"{base_dir}/test_set_2\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)  # Save training set\n",
    "#     pd.DataFrame(validation).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)  # Save validation set\n",
    "#     pd.DataFrame(test_set_1).to_csv(f\"{base_dir}/test_set_1/test_set_1.csv\", header=False, index=False)  # Save test_set_1\n",
    "#     pd.DataFrame(test_set_2).to_csv(f\"{base_dir}/test_set_2/test_set_2.csv\", header=False, index=False)  # Save test_set_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af685456-4be9-4ffa-a551-79659969b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocess.py\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# Define column names and data types for the Auto MPG dataset\n",
    "feature_columns_names = [\n",
    "    \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"origin\"\n",
    "]\n",
    "label_column = \"mpg\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"cylinders\": np.float64,  # Change to float64 to handle NA values before converting to int\n",
    "    \"displacement\": np.float64,\n",
    "    \"horsepower\": np.float64,\n",
    "    \"weight\": np.float64,\n",
    "    \"acceleration\": np.float64,\n",
    "    \"model_year\": np.float64,  # Change to float64 to handle NA values before converting to int\n",
    "    \"origin\": str,\n",
    "}\n",
    "label_column_dtype = {\"mpg\": np.float64}\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    \"\"\"Merges two dicts, returning a new copy.\"\"\"\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.debug(\"Starting preprocessing.\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input-data\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    pathlib.Path(f\"{base_dir}/data\").mkdir(parents=True, exist_ok=True)  # Create directories for input and output\n",
    "    input_data = args.input_data\n",
    "    bucket = input_data.split(\"/\")[2]\n",
    "    key = \"/\".join(input_data.split(\"/\")[3:])\n",
    "\n",
    "    logger.info(\"Downloading data from bucket: %s, key: %s\", bucket, key)\n",
    "    fn = f\"{base_dir}/data/auto-mpg-dataset.csv\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    s3.Bucket(bucket).download_file(key, fn)  # Download the dataset from S3\n",
    "\n",
    "    logger.debug(\"Reading downloaded data.\")\n",
    "    df = pd.read_csv(fn, header=None, names=feature_columns_names)  # Read the CSV file into a DataFrame\n",
    "    os.unlink(fn)  # Delete the local file after reading\n",
    "\n",
    "    # Ensure 'horsepower' is handled correctly\n",
    "    logger.debug(\"Handling missing values in 'horsepower' column.\")\n",
    "    df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')  # Convert 'horsepower' to numeric, coerce errors to NaN\n",
    "\n",
    "    # Convert non-numeric values to NaN\n",
    "    for col in feature_columns_dtype.keys():\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Handle missing values before converting data types\n",
    "    df = df.fillna(df.median())  # Fill NaN values with the median\n",
    "\n",
    "    # Convert columns to the appropriate data types\n",
    "    for col, dtype in feature_columns_dtype.items():\n",
    "        df[col] = df[col].astype(dtype)\n",
    "\n",
    "    logger.debug(\"Defining transformers.\")\n",
    "    features = list(feature_columns_names)\n",
    "    features.remove(label_column)  # Remove the label column from the features list\n",
    "\n",
    "    numeric_features = [name for name in features if df.dtypes[name] != 'object']  # Identify numeric features\n",
    "    categorical_features = [name for name in features if df.dtypes[name] == 'object']  # Identify categorical features\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values with median\n",
    "            (\"scaler\", StandardScaler()),  # Standardize the numeric features\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),  # Impute missing values with 'missing'\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),  # One-hot encode categorical features\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),  # Apply numeric transformations\n",
    "            (\"cat\", categorical_transformer, categorical_features),  # Apply categorical transformations\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger.info(\"Applying transforms.\")\n",
    "    y = df.pop(label_column)  # Separate the label from the features\n",
    "    X_pre = preprocess.fit_transform(df)  # Fit and transform the features\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)  # Reshape the label array\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)  # Concatenate the label and features\n",
    "\n",
    "    logger.info(\"Splitting %d rows of data into train, validation, test datasets.\", len(X))\n",
    "    np.random.shuffle(X)  # Shuffle the dataset\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])  # Split the dataset\n",
    "\n",
    "    # Further split the test set into test_set_1 and test_set_2\n",
    "    test_set_1, test_set_2 = np.split(test, [int(0.5 * len(test))])\n",
    "\n",
    "    logger.info(\"Writing out datasets to %s.\", base_dir)\n",
    "    pathlib.Path(f\"{base_dir}/train\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(f\"{base_dir}/validation\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(f\"{base_dir}/test_set_1\").mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(f\"{base_dir}/test_set_2\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)  # Save training set\n",
    "    pd.DataFrame(validation).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)  # Save validation set\n",
    "    pd.DataFrame(test_set_1).to_csv(f\"{base_dir}/test_set_1/test_set_1.csv\", header=False, index=False)  # Save test_set_1\n",
    "\n",
    "    # Remove the label column from test_set_2\n",
    "    test_set_2_features = test_set_2[:, 1:]\n",
    "    pd.DataFrame(test_set_2_features).to_csv(f\"{base_dir}/test_set_2/test_set_2.csv\", header=False, index=False)  # Save test_set_2 without the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89898234-a08e-4d3e-8d57-680bd72f7498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "Defaulting to only available Python version: py3\n",
      "Defaulting to only available Python version: py3\n",
      "/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "# Initialize the SKLearnProcessor for processing the Auto MPG dataset\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",  # Version of the scikit-learn framework\n",
    "    instance_type=\"ml.m5.xlarge\",  # Type of instance to use\n",
    "    instance_count=processing_instance_count,  # Number of instances to use\n",
    "    base_job_name=f\"{base_job_prefix}/sklearn-autompg-preprocess\",  # Base name for the processing job\n",
    "    sagemaker_session=pipeline_session,  # SageMaker session\n",
    "    role=role,  # IAM role for the processing job\n",
    ")\n",
    "\n",
    "# Define the run arguments for the processing job\n",
    "processor_run_args = sklearn_processor.run(\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",  # Name of the output\n",
    "            source=\"/opt/ml/processing/train\",  # Source directory in the container\n",
    "            destination=Join(  # Destination S3 URI\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\",\n",
    "                    default_bucket,\n",
    "                    base_job_prefix,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,  # Use pipeline execution ID for dynamic output path\n",
    "                    \"PreprocessAutoMpgData\",  # Folder name for train data\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\",  # Name of the output\n",
    "            source=\"/opt/ml/processing/validation\",  # Source directory in the container\n",
    "            destination=Join(  # Destination S3 URI\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\",\n",
    "                    default_bucket,\n",
    "                    base_job_prefix,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,  # Use pipeline execution ID for dynamic output path\n",
    "                    \"PreprocessAutoMpgData\",  # Folder name for validation data\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test_set_1\",  # Name of the output\n",
    "            source=\"/opt/ml/processing/test_set_1\",  # Source directory in the container\n",
    "            destination=Join(  # Destination S3 URI\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\",\n",
    "                    default_bucket,\n",
    "                    base_job_prefix,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,  # Use pipeline execution ID for dynamic output path\n",
    "                    \"PreprocessAutoMpgData\",  # Folder name for test set 1\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test_set_2\",  # Name of the output\n",
    "            source=\"/opt/ml/processing/test_set_2\",  # Source directory in the container\n",
    "            destination=Join(  # Destination S3 URI\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3:/\",\n",
    "                    default_bucket,\n",
    "                    base_job_prefix,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,  # Use pipeline execution ID for dynamic output path\n",
    "                    \"PreprocessAutoMpgData\",  # Folder name for test set 2\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    code=\"code/preprocess.py\",  # Path to the preprocessing script\n",
    "    arguments=[\"--input-data\", input_data],  # Arguments to pass to... the script\n",
    ")\n",
    "\n",
    "# Define the processing step for the pipeline\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessAutoMpgData\",  \n",
    "    step_args=processor_run_args,  # Arguments for the processing step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8efbc513-7bbf-4d51-9403-db2feee75cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator  \n",
    "from sagemaker.inputs import TrainingInput  \n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter  \n",
    "from sagemaker.workflow.steps import TuningStep  \n",
    "from sagemaker.workflow.functions import Join  \n",
    "from sagemaker.image_uris import retrieve  \n",
    "\n",
    "# Define the output path for the model artifacts from the Hyperparameter Tuning Job\n",
    "model_path = f\"s3://{default_bucket}/{base_job_prefix}/AutoMpgTrain\"\n",
    "\n",
    "# Retrieve the image URI for the XGBoost algorithm\n",
    "image_uri = retrieve(\n",
    "    framework=\"xgboost\",  # Specify the framework as XGBoost\n",
    "    region=region,  \n",
    "    version=\"1.0-1\",  # Specify the version of XGBoost\n",
    "    py_version=\"py3\",  # Specify the Python version\n",
    "    instance_type=\"ml.m5.xlarge\",  \n",
    ")\n",
    "\n",
    "# Initialize the XGBoost estimator\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,  # The URI of the container image for XGBoost\n",
    "    instance_type=instance_type,  # The type of EC2 instance to use for training\n",
    "    instance_count=1,  \n",
    "    output_path=model_path,  # The S3 location where the model artifacts will be saved\n",
    "    base_job_name=f\"{base_job_prefix}/autompg-train\",  # The base name for the training job\n",
    "    sagemaker_session=pipeline_session,  # The SageMaker session to use\n",
    "    role=role,  # The IAM role for the training job\n",
    ")\n",
    "\n",
    "# Set the hyperparameters for the XGBoost model\n",
    "xgb_train.set_hyperparameters(\n",
    "    eval_metric=\"rmse\",  # The evaluation metric to use\n",
    "    objective=\"reg:squarederror\",  # The objective metric for the training job\n",
    "    num_round=50,  # The number of boosting rounds\n",
    "    max_depth=5,  # The maximum depth of a tree\n",
    "    eta=0.2,  # The step size shrinkage\n",
    "    gamma=4,  # The minimum loss reduction required to make a further partition\n",
    "    min_child_weight=6,  # The minimum sum of instance weight needed in a child\n",
    "    subsample=0.7,  # The subsample ratio of the training instances\n",
    "    silent=0,  # The logging mode\n",
    ")\n",
    "\n",
    "# Define the objective metric for the Hyperparameter Tuning Job\n",
    "objective_metric_name = \"validation:rmse\"\n",
    "\n",
    "# Define the hyperparameter ranges to tune\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),  # The range for the alpha parameter\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),  # The range for the lambda parameter\n",
    "}\n",
    "\n",
    "# Initialize the Hyperparameter Tuner\n",
    "tuner_log = HyperparameterTuner(\n",
    "    xgb_train,  # The XGBoost estimator to tune\n",
    "    objective_metric_name,  # The objective metric to optimize\n",
    "    hyperparameter_ranges,  # The ranges of hyperparameters to tune\n",
    "    max_jobs=3,  # The maximum number of jobs to run in total\n",
    "    max_parallel_jobs=3,  # The maximum number of jobs to run in parallel\n",
    "    strategy=\"Bayesian\",  # The search strategy for hyperparameter tuning\n",
    "    objective_type=\"Minimize\",  # The type of objective metric (minimize or maximize)\n",
    ")\n",
    "\n",
    "# Define the inputs for the Hyperparameter Tuning Job\n",
    "hpo_args = tuner_log.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,  # S3 URI for training data\n",
    "            content_type=\"text/csv\",  # Content type of the training data\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,  # S3 URI for validation data\n",
    "            content_type=\"text/csv\",  # Content type of the validation data\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Defining the Tuning Step for the pipeline\n",
    "step_tuning = TuningStep(\n",
    "    name=\"HPTuning\",  \n",
    "    step_args=hpo_args,  # The arguments for the tuning step\n",
    "    cache_config=cache_config,  # The cache configuration to reduce execution time on subsequent executions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ef8879e-98d1-498d-9f0c-2a48e0f82e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SageMaker Model\n",
    "model_prefix = f\"{base_job_prefix}/AutoMpgTrain\"  # Define the prefix for the model path in S3\n",
    "\n",
    "# Initialize the Model object for the best model\n",
    "best_model = Model(\n",
    "    image_uri=image_uri,                          # URI of the container image for the model\n",
    "    model_data=step_tuning.get_top_model_s3_uri(\n",
    "        top_k=0,                                  # Get the top model from the hyperparameter tuning job\n",
    "        s3_bucket=default_bucket,                 # The S3 bucket where the model artifacts are stored\n",
    "        prefix=model_prefix                       # The prefix for the model path in S3\n",
    "    ),\n",
    "    predictor_cls=XGBoostPredictor,               # The predictor class to use for deployment\n",
    "    sagemaker_session=pipeline_session,           # The SageMaker session to use\n",
    "    role=role,                                    # The IAM role for the model\n",
    ")\n",
    "\n",
    "# Define the model creation step in the pipeline\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateBestModel\",  \n",
    "    step_args=best_model.create(instance_type=\"ml.m5.xlarge\"),  # Arguments for creating the model and specifying the instance type to use\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "663d8b87-f51b-4480-9dae-f6d6e47398b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluate.py\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"  # Path to the tarred model file\n",
    "    with tarfile.open(model_path) as tar:  # Extract the tarred model file\n",
    "        tar.extractall(path=\".\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))  # Load the XGBoost model from the extracted file\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test_set_1/test_set_1.csv\"  # Path to the test dataset\n",
    "    df = pd.read_csv(test_path, header=None)  # Read the test dataset into a DataFrame\n",
    "    y_test = df.iloc[:, 0].to_numpy()  # Extract the labels (assuming the label is the first column)\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)  # Drop the label column from the test data\n",
    "    dmatrix_test = xgboost.DMatrix(df.values)  # Create DMatrix for XGBoost prediction\n",
    "\n",
    "    predictions = model.predict(dmatrix_test)  # Make predictions using the loaded model\n",
    "    mse = mean_squared_error(y_test, predictions)  # Calculate mean squared error\n",
    "    std = np.std(y_test - predictions)  # Calculate standard deviation of errors\n",
    "\n",
    "    report_dict = {  # Create a dictionary to store the evaluation metrics\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"                     # Define the output directory for the evaluation report\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)      # Create the output directory if it doesn't exist\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"                # Path to the evaluation report JSON file\n",
    "    with open(evaluation_path, \"w\") as f:                            # Write the evaluation metrics to the JSON file\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f1f7bb0-9c09-4545-80fa-2f83fde2a7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,  # Docker image URI for the processing job\n",
    "    command=[\"python3\"],  # Command to run\n",
    "    instance_type=\"ml.m5.xlarge\",  \n",
    "    instance_count=1,  \n",
    "    base_job_name=f\"{base_job_prefix}/script-autompg-eval\",  # Base name for the processing job\n",
    "    sagemaker_session=pipeline_session,  \n",
    "    role=role,  \n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"AutoMPGEvaluationReport\",  # Name of the property file\n",
    "    output_name=\"evaluation\",  # Output name\n",
    "    path=\"evaluation.json\",  # Path to the evaluation JSON file\n",
    ")\n",
    "\n",
    "processor_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_tuning.get_top_model_s3_uri(\n",
    "                top_k=0, s3_bucket=default_bucket, prefix=model_prefix     # Source S3 URI for the model artifact\n",
    "            ), \n",
    "            destination=\"/opt/ml/processing/model\",  # Destination directory in the container\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test_set_1\"].S3Output.S3Uri,  # Source S3 URI for the test data\n",
    "            destination=\"/opt/ml/processing/test_set_1\",  # Destination directory in the container\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),  # Output directory in the container\n",
    "    ],\n",
    "    code=\"code/evaluate.py\",  # Path to the evaluation script\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EvaluateTopModel\",  \n",
    "    step_args=processor_args,  # Arguments for the processing step\n",
    "    property_files=[evaluation_report],  # Property file for the evaluation report\n",
    "    cache_config=cache_config,  # Cache configuration\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]     # S3 URI for the evaluation JSON file\n",
    "        ),  \n",
    "        content_type=\"application/json\",  # Content type of the evaluation file\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6931d1f9-aa8f-4fe5-ad50-6fd056ddb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "# Define the Transformer\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,  # Name of the model created in the previous step\n",
    "    instance_type=\"ml.m5.xlarge\",  # Type of instance to use for the transform job\n",
    "    instance_count=1,  # Number of instances to use for the transform job\n",
    "    output_path=f\"s3://{default_bucket}/AutoMPGTransform\",  # S3 path where the transform output will be stored\n",
    ")\n",
    "\n",
    "# Define the TransformInput\n",
    "batch_data = Join(\n",
    "    on=\"/\",\n",
    "    values=[\n",
    "        f\"s3://{default_bucket}\",\n",
    "        \"ab3-autompg-example\",\n",
    "        ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "        \"PreprocessAutoMpgData\",\n",
    "        \"test_set_2.csv\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the TransformStep\n",
    "step_transform = TransformStep(\n",
    "    name=\"AutoMPGTransform\",  # Name of the transform step\n",
    "    transformer=transformer,  # Transformer object defined above\n",
    "    inputs=TransformInput(data=batch_data, content_type=\"text/csv\"),  # Input data for the transform job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "477e2da0-950f-4e95-8acf-6ee9763f98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "# Define the model registration arguments\n",
    "register_args = best_model.register(\n",
    "    content_types=[\"text/csv\"],  # Define the content types for the model input\n",
    "    response_types=[\"text/csv\"],  # Define the response types for the model output\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],  # Define the instance types for inference\n",
    "    transform_instances=[\"ml.m5.large\"],  # Define the instance types for batch transformation\n",
    "    model_package_group_name=model_package_group_name,  # Define the model package group name\n",
    "    approval_status=model_approval_status,  # Define the approval status of the model\n",
    ")\n",
    "\n",
    "# Define the model registration step\n",
    "step_register = ModelStep(\n",
    "    name=\"RegisterBestAutoMPGModel\",  # Name of the model registration step\n",
    "    step_args=register_args,  # Arguments for registering the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe6930bf-f109-4ddf-a3fb-facb3785d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "# Define the fail step\n",
    "step_fail = FailStep(\n",
    "    name=\"AutoMPGMSEFail\",  # Name of the fail step\n",
    "    error_message=Join(\n",
    "        on=\" \",  # Join elements with a space\n",
    "        values=[\"Execution failed due to MSE >\", mse_threshold]  # Error message parts\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f805e03-bdb0-4067-bc86-c08f88a615c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "# Create a condition that checks if the MSE is less than or equal to the threshold\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,  # Name of the evaluation step\n",
    "        property_file=evaluation_report,  # Property file containing the evaluation metrics\n",
    "        json_path=\"regression_metrics.mse.value\",  # JSON path to the MSE value in the evaluation report\n",
    "    ),\n",
    "    right=mse_threshold,  # The threshold value for the condition\n",
    ")\n",
    "\n",
    "# Define the condition step\n",
    "step_cond = ConditionStep(\n",
    "    name=\"AutoMPGMSECond\",  # Name of the condition step\n",
    "    conditions=[cond_lte],  # List of conditions to check\n",
    "    if_steps=[step_register, step_create_model, step_transform],  # Steps to execute if the condition is met\n",
    "    else_steps=[step_fail],  # Steps to execute if the condition is not met (e.g., logging or notifications)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5936a891-5c7b-45e8-a62e-956c80563753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "# Defining the pipeline\n",
    "\n",
    "pipeline_name = \"AutoMPGPipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_tuning, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e27f944a-7a3a-4fa6-a67a-7fe11be4f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceCount',\n",
       "   'Type': 'Integer',\n",
       "   'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-211343875790/auto-mpg/auto-mpg-dataset.csv'},\n",
       "  {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 46.0}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'PreprocessAutoMpgData',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3',\n",
       "     'ContainerArguments': ['--input-data', {'Get': 'Parameters.InputData'}],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocess.py']},\n",
       "    'RoleArn': 'arn:aws:iam::211343875790:role/service-role/AmazonSageMaker-ExecutionRole-20240218T181424',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-211343875790/AutoMPGPipeline/code/70159ae499944480885b0811331cc4d9/preprocess.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-211343875790',\n",
       "           'ab3-autompg-example',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAutoMpgData']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-211343875790',\n",
       "           'ab3-autompg-example',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAutoMpgData']}},\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test_set_1',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-211343875790',\n",
       "           'ab3-autompg-example',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAutoMpgData']}},\n",
       "        'LocalPath': '/opt/ml/processing/test_set_1',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test_set_2',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-211343875790',\n",
       "           'ab3-autompg-example',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'PreprocessAutoMpgData']}},\n",
       "        'LocalPath': '/opt/ml/processing/test_set_2',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'HPTuning',\n",
       "   'Type': 'Tuning',\n",
       "   'Arguments': {'HyperParameterTuningJobConfig': {'Strategy': 'Bayesian',\n",
       "     'ResourceLimits': {'MaxNumberOfTrainingJobs': 3,\n",
       "      'MaxParallelTrainingJobs': 3},\n",
       "     'TrainingJobEarlyStoppingType': 'Off',\n",
       "     'HyperParameterTuningJobObjective': {'Type': 'Minimize',\n",
       "      'MetricName': 'validation:rmse'},\n",
       "     'ParameterRanges': {'ContinuousParameterRanges': [{'Name': 'alpha',\n",
       "        'MinValue': '0.01',\n",
       "        'MaxValue': '10',\n",
       "        'ScalingType': 'Logarithmic'},\n",
       "       {'Name': 'lambda',\n",
       "        'MinValue': '0.01',\n",
       "        'MaxValue': '10',\n",
       "        'ScalingType': 'Logarithmic'}],\n",
       "      'CategoricalParameterRanges': [],\n",
       "      'IntegerParameterRanges': []}},\n",
       "    'TrainingJobDefinition': {'StaticHyperParameters': {'eval_metric': 'rmse',\n",
       "      'objective': 'reg:squarederror',\n",
       "      'num_round': '50',\n",
       "      'max_depth': '5',\n",
       "      'eta': '0.2',\n",
       "      'gamma': '4',\n",
       "      'min_child_weight': '6',\n",
       "      'subsample': '0.7',\n",
       "      'silent': '0'},\n",
       "     'RoleArn': 'arn:aws:iam::211343875790:role/service-role/AmazonSageMaker-ExecutionRole-20240218T181424',\n",
       "     'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-211343875790/ab3-autompg-example/AutoMpgTrain'},\n",
       "     'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "     'HyperParameterTuningResourceConfig': {'InstanceCount': 1,\n",
       "      'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "      'VolumeSizeInGB': 30},\n",
       "     'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "      'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3'},\n",
       "     'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "         'S3Uri': {'Get': \"Steps.PreprocessAutoMpgData.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "         'S3DataDistributionType': 'FullyReplicated'}},\n",
       "       'ContentType': 'text/csv',\n",
       "       'ChannelName': 'train'},\n",
       "      {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "         'S3Uri': {'Get': \"Steps.PreprocessAutoMpgData.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "         'S3DataDistributionType': 'FullyReplicated'}},\n",
       "       'ContentType': 'text/csv',\n",
       "       'ChannelName': 'validation'}]}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': '30d'}},\n",
       "  {'Name': 'EvaluateTopModel',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluate.py']},\n",
       "    'RoleArn': 'arn:aws:iam::211343875790:role/service-role/AmazonSageMaker-ExecutionRole-20240218T181424',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "         'Values': ['s3:/',\n",
       "          'sagemaker-us-east-1-211343875790',\n",
       "          'ab3-autompg-example/AutoMpgTrain',\n",
       "          {'Get': 'Steps.HPTuning.TrainingJobSummaries[0].TrainingJobName'},\n",
       "          'output/model.tar.gz']}},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.PreprocessAutoMpgData.ProcessingOutputConfig.Outputs['test_set_1'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test_set_1',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-211343875790/AutoMPGPipeline/code/546df4a0e455be90b3ffbd4d37b09e0a/evaluate.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211343875790/ab3-autompg-example/script-autompg-eval-2024-06-20-02-00-53-525/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': '30d'},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'AutoMPGEvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'AutoMPGMSECond',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo',\n",
       "      'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.EvaluateTopModel.PropertyFiles.AutoMPGEvaluationReport'},\n",
       "        'Path': 'regression_metrics.mse.value'}},\n",
       "      'RightValue': {'Get': 'Parameters.MseThreshold'}}],\n",
       "    'IfSteps': [{'Name': 'RegisterBestAutoMPGModel-RegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'AutoMpgPackageGroupName',\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "          'Environment': {},\n",
       "          'ModelDataUrl': {'Std:Join': {'On': '/',\n",
       "            'Values': ['s3:/',\n",
       "             'sagemaker-us-east-1-211343875790',\n",
       "             'ab3-autompg-example/AutoMpgTrain',\n",
       "             {'Get': 'Steps.HPTuning.TrainingJobSummaries[0].TrainingJobName'},\n",
       "             'output/model.tar.gz']}}}],\n",
       "        'SupportedContentTypes': ['text/csv'],\n",
       "        'SupportedResponseMIMETypes': ['text/csv'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium',\n",
       "         'ml.m5.large'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.m5.large']},\n",
       "       'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
       "       'SkipModelValidation': 'None'}},\n",
       "     {'Name': 'CreateBestModel-CreateModel',\n",
       "      'Type': 'Model',\n",
       "      'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::211343875790:role/service-role/AmazonSageMaker-ExecutionRole-20240218T181424',\n",
       "       'PrimaryContainer': {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "        'Environment': {},\n",
       "        'ModelDataUrl': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-us-east-1-211343875790',\n",
       "           'ab3-autompg-example/AutoMpgTrain',\n",
       "           {'Get': 'Steps.HPTuning.TrainingJobSummaries[0].TrainingJobName'},\n",
       "           'output/model.tar.gz']}}}}},\n",
       "     {'Name': 'AutoMPGTransform',\n",
       "      'Type': 'Transform',\n",
       "      'Arguments': {'ModelName': {'Get': 'Steps.CreateBestModel-CreateModel.ModelName'},\n",
       "       'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "          'S3Uri': {'Std:Join': {'On': '/',\n",
       "            'Values': ['s3://sagemaker-us-east-1-211343875790',\n",
       "             'ab3-autompg-example',\n",
       "             {'Get': 'Execution.PipelineExecutionId'},\n",
       "             'PreprocessAutoMpgData',\n",
       "             'test_set_2.csv']}}}},\n",
       "        'ContentType': 'text/csv'},\n",
       "       'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-east-1-211343875790/AutoMPGTransform'},\n",
       "       'TransformResources': {'InstanceCount': 1,\n",
       "        'InstanceType': 'ml.m5.xlarge'}}}],\n",
       "    'ElseSteps': [{'Name': 'AutoMPGMSEFail',\n",
       "      'Type': 'Fail',\n",
       "      'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
       "         'Values': ['Execution failed due to MSE >',\n",
       "          {'Get': 'Parameters.MseThreshold'}]}}}}]}}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3122846f-e5ca-4d6c-9612-4eff60c16cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:211343875790:pipeline/AutoMPGPipeline',\n",
       " 'ResponseMetadata': {'RequestId': '558b5bfa-da37-4dd4-918d-998703c97d94',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '558b5bfa-da37-4dd4-918d-998703c97d94',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '83',\n",
       "   'date': 'Thu, 20 Jun 2024 02:01:05 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd9fbd90-83e3-4c4a-b256-5ca8088555bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8d1de1d-b28d-44a3-993f-fe6d4d09f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8364019-513f-4f95-bb53-3f796901f3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regression_metrics': {'mse': {'standard_deviation': 0.23584178988203086,\n",
      "                                'value': 0.06323755908057743}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec70cc4-9362-4eb3-98ff-d3a190952aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
